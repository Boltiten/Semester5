{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "# 1) Design model (input, output size, forward pass)\r\n",
    "# 2) Construct loss and optimizer\r\n",
    "# 3) training loop\r\n",
    "#       - Forward pass: compute prediction\r\n",
    "#       - Backward pass: gradients\r\n",
    "#       - update weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import numpy as np\r\n",
    "import matplotlib as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32).reshape(-1,1)\r\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32).reshape(-1,1)\r\n",
    "\r\n",
    "X_test = torch.tensor([5],dtype=torch.float32)\r\n",
    "n_samples, n_features = X.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "# model prediction\r\n",
    "\r\n",
    "input_size = n_features\r\n",
    "output_size = n_features\r\n",
    "\r\n",
    "class LinearRegression(nn.Module):\r\n",
    "    def __init__(self, input_dim, output_dim):\r\n",
    "        super(LinearRegression, self).__init__()\r\n",
    "        #define layers\r\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.lin(x)\r\n",
    "\r\n",
    "model = LinearRegression(input_size, output_size)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "print(f'Prediction before training: f(5) = {model(X_test).item()}')\r\n",
    "#Training\r\n",
    "learning_rate = 0.01\r\n",
    "it = 5000\r\n",
    "\r\n",
    "loss = nn.MSELoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "\r\n",
    "for epoch in range(it):\r\n",
    "    #prediction = forward pass\r\n",
    "    y_pred = model(X)\r\n",
    "\r\n",
    "    #loss\r\n",
    "    l = loss(Y,y_pred)\r\n",
    "\r\n",
    "    #gradients = backward pass\r\n",
    "    l.backward() # dl/dw\r\n",
    "\r\n",
    "    #update weights\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    # zero gradient\r\n",
    "    optimizer.zero_grad()\r\n",
    "\r\n",
    "    if epoch % 500 == 0:\r\n",
    "        [w,b] = model.parameters()\r\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\r\n",
    "\r\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction before training: f(5) = -1.4166738986968994\n",
      "epoch 1: w = 0.265, loss = 43.62630844\n",
      "epoch 501: w = 2.015, loss = 0.00032484\n",
      "epoch 1001: w = 2.003, loss = 0.00001620\n",
      "epoch 1501: w = 2.001, loss = 0.00000081\n",
      "epoch 2001: w = 2.000, loss = 0.00000004\n",
      "epoch 2501: w = 2.000, loss = 0.00000000\n",
      "epoch 3001: w = 2.000, loss = 0.00000000\n",
      "epoch 3501: w = 2.000, loss = 0.00000000\n",
      "epoch 4001: w = 2.000, loss = 0.00000000\n",
      "epoch 4501: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8b7a5e7e088dc87b16d72ecb84aa6dc0c370a308eb78f8481e88a4680c33e6f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}