{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2A. Q-Læring: Gridworld med visualisering\n",
    "Lag et enkelt gridworld-environment. Dette innebærer at environmentet har et\n",
    "diskret rutenett, og at en agent kan bevege seg rundt med fire handlinger (opp,\n",
    "ned, høyre, venstre). Simuleringen terminerer n˚ar agenten har n˚add et plassert\n",
    "m˚al-posisjon som gir reward 1. Om man ønsker, kan det legges inn f.eks. solide\n",
    "vegger eller farlige omr˚ader som gir straff rundt omkring. Environmentet skal\n",
    "ha samme interface som cartpole (.step(a)-funksjon, og .reset())\n",
    "Deretter skal implementasjonen av Q-læring fra forrige oppgave brukes for ˚a\n",
    "trene en agent i environmentet. Til slutt skal Q-verdiene visualiseres inne i selve\n",
    "environmentet, og dette kan gjøres p˚a flere m˚ater. En m˚ate er˚a fargelegge rutene\n",
    "basert p˚a den høyeste Q-verdien fra tilsvarende rad i Q-tabellen. Alternativt s˚a\n",
    "kan man tegne inn piler som peker i samme retning som handlingen med høyest\n",
    "Q-verdi.\n",
    "Tips: Biblioteket pygame er veldig greit for ˚a lage visualisering av environmentet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGrid(screen, color, goal, user, q_table):\n",
    "    blockSize = 60\n",
    "    use_color = color\n",
    "    scalar = 2.5 if np.max(q_table) < 2 else 1\n",
    "    for x in range(0, 600, blockSize):\n",
    "        for y in range(0, 600, blockSize):\n",
    "            use_color = color\n",
    "            \n",
    "            rect = pygame.Rect(x, y, blockSize, blockSize)\n",
    "\n",
    "            q_val = (np.max(q_table[(int(x/60), int(y/60))]) + 1)*scalar\n",
    "            if q_val > 0:\n",
    "                temp = np.array(use_color)\n",
    "                temp[0] = 255\n",
    "                temp[1] = 255 - int(25*q_val)\n",
    "                temp[2] = 0\n",
    "                pygame.draw.rect(screen, tuple(temp), rect)\n",
    "\n",
    "            if (x, y) == tuple(np.multiply(goal,60)):\n",
    "                pygame.draw.rect(screen, (0, 255, 0), rect, 60, 60)\n",
    "            if (x, y) == tuple(np.multiply(user,60)):\n",
    "                pygame.draw.rect(screen, (0, 0, 255), rect, 60, 60)\n",
    "\n",
    "            pygame.draw.rect(screen, use_color, rect, 1)\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.start_pos = np.random.randint(5,9,size=2)\n",
    "        self.rewards = np.zeros((10,10))\n",
    "        self.goal = np.array([0, 0])\n",
    "        self.reset()\n",
    "        self.calculate_rewards()\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "        self.player = np.array(self.start_pos)\n",
    "        return tuple(self.player)\n",
    "\n",
    "    def calculate_rewards(self):\n",
    "        for x in range(len(self.rewards)):\n",
    "            for y in range(len(self.rewards[0])):\n",
    "                self.rewards[x, y] = 1 - self.manhatten_distance([x, y])**0.4\n",
    "    \n",
    "    def random_action(self):\n",
    "        random.randint(0,4)\n",
    "\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        # 0: UP, 1: RIGHT, 2: DOWN, 3: LEFT\n",
    "        done = tuple(self.player) == tuple(self.goal)\n",
    "        reward = -1\n",
    "        if (self.valid_move(action)):\n",
    "            self.make_new_state(action)\n",
    "            reward = self.rewards[tuple(self.player)]\n",
    "\n",
    "            \n",
    "\n",
    "        return tuple(self.player), reward, done\n",
    "\n",
    "    def manhatten_distance(self, node):\n",
    "        return abs(self.goal[0] - node[0]) + abs(self.goal[1] - node[1])\n",
    "    \n",
    "    def make_new_state(self, action):\n",
    "        # 0: UP, 1: RIGHT, 2: DOWN, 3: LEFT\n",
    "        if(action == 0): self.player[1] -= 1\n",
    "        elif(action == 1): self.player[0] += 1\n",
    "        elif(action == 2): self.player[1] += 1\n",
    "        elif(action == 3): self.player[0] -= 1\n",
    "\n",
    "    def valid_move(self, action):\n",
    "        if(action == 0): return self.player[1] > 0\n",
    "        elif(action == 1): return self.player[0] < 9\n",
    "        elif(action == 2): return self.player[1] < 9\n",
    "        elif(action == 3): return self.player[0] > 0\n",
    "        return False\n",
    "    \n",
    "    def render(self, q_table):\n",
    "        pygame.init()\n",
    "\n",
    "        screen = pygame.display.set_mode([600, 600])\n",
    "        pygame.display.set_caption(\"Gridworld\")\n",
    "\n",
    "        black = (0, 0, 0)\n",
    "        white = (255, 255, 255)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:  # If user clicked close\n",
    "                    done = True\n",
    "\n",
    "            state = self.reset()\n",
    "\n",
    "            done1  = False\n",
    "            while not done1:\n",
    "                action = np.argmax(q_table[state])  \n",
    "                state, _, done1 = self.step(action)\n",
    "                sleep(0.1)\n",
    "                screen.fill(black)\n",
    "                drawGrid(screen, white, self.goal, self.player, q_table)\n",
    "                pygame.display.flip()\n",
    "            \n",
    "\n",
    "        pygame.quit()\n",
    "    \n",
    "    def render_train(self):\n",
    "        pygame.init()\n",
    "\n",
    "        q_table = np.random.uniform(low=-1, high=1, size=([len(self.rewards)] * 2 + [4]))\n",
    "        lr = 0.1\n",
    "        gamma = 0.95\n",
    "        epsilon = 0.2\n",
    "\n",
    "        screen = pygame.display.set_mode([600, 600])\n",
    "        pygame.display.set_caption(\"Gridworld\")\n",
    "\n",
    "        black = (0, 0, 0)\n",
    "        white = (255, 255, 255)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:  # If user clicked close\n",
    "                    done = True\n",
    "\n",
    "            state = self.reset()\n",
    "\n",
    "            done1  = False\n",
    "            while not done1:\n",
    "                if(random.uniform(0, 1) < epsilon):\n",
    "                    action = self.random_action()\n",
    "                else:\n",
    "                    action = np.argmax(q_table[state])\n",
    "\n",
    "                next_state, reward, done1 = self.step(action)\n",
    "\n",
    "                old_value = q_table[state + (action, )]\n",
    "                next_max = np.max(q_table[next_state])\n",
    "                new_value = (1-lr)*old_value + lr * (reward + gamma * next_max )\n",
    "\n",
    "                q_table[state + (action, )] = new_value\n",
    "                state = next_state\n",
    "                sleep(0.001)\n",
    "                screen.fill(black)\n",
    "                drawGrid(screen, white, self.goal, self.player, q_table)\n",
    "                pygame.display.flip()\n",
    "            \n",
    "\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "q_table = np.random.uniform(low=-1, high=1, size=([len(env.rewards)] * 2 + [4]))\n",
    "\n",
    "lr = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs = []\n",
    "for i in range(1, 1500):\n",
    "\tstate = env.reset()\n",
    "\n",
    "\tepochs, reward = 0,0\n",
    "\taction = -1\n",
    "\tdone  = False\n",
    "\twhile not done:\n",
    "\t\tif(random.uniform(0, 1) < epsilon):\n",
    "\t\t\taction = env.random_action()\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])\n",
    "\t\t\n",
    "\t\tnext_state, reward, done = env.step(action)\n",
    "\t\t\n",
    "\t\told_value = q_table[state + (action, )]\n",
    "\t\tnext_max = np.max(q_table[next_state])\n",
    "\t\tnew_value = (1-lr)*old_value + lr * (reward + gamma * next_max )\n",
    "\n",
    "\t\tq_table[state + (action, )] = new_value\n",
    "\t\tstate = next_state\n",
    "\t\tepochs +=1\n",
    "\n",
    "\tall_epochs.append(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-04a0a4c6825e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-52cd3cf985a8>\u001b[0m in \u001b[0;36mrender_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[0mdrawGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m                 \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-52cd3cf985a8>\u001b[0m in \u001b[0;36mdrawGrid\u001b[1;34m(screen, color, goal, user, q_table)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mq_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mq_val\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_color\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2703\u001b[0m     \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m-> 2705\u001b[1;33m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[0;32m   2706\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   2707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.render_train()\n",
    "env.render(q_table)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73cdb8068fb29a4d992b50a8130c5699128e49a23c234139f6f5107947219a38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
