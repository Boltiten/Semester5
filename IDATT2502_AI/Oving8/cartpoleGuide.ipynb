{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np \n",
    "import time, math, random\n",
    "from typing import Tuple\n",
    "\n",
    "# import gym \n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = lambda _,__,___, tip_velocity : int( tip_velocity > 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = ( 6 , 12 )\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy( state : tuple ):\n",
    "    \"\"\"Choosing action based on epsilon-greedy policy\"\"\"\n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    \"\"\"Temperal diffrence for updating Q-value of state-action pair\"\"\"\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive learning of Learning Rate\n",
    "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
    "    \"\"\"Decaying learning rate\"\"\"\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
    "    \"\"\"Decaying exploration rate\"\"\"\n",
    "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Time: 0.03680012226104736\n",
      "Episode: 2 Time: 0.02330038547515869\n",
      "Episode: 3 Time: 0.03650012016296387\n",
      "Episode: 4 Time: 0.028299999237060548\n",
      "Episode: 5 Time: 0.02649989128112793\n",
      "Episode: 6 Time: 0.014899682998657227\n",
      "Episode: 7 Time: 0.030000019073486327\n",
      "Episode: 8 Time: 0.023799991607666014\n",
      "Episode: 9 Time: 0.07169952392578124\n",
      "Episode: 10 Time: 0.031599569320678714\n",
      "Episode: 11 Time: 0.026599955558776856\n",
      "Episode: 12 Time: 0.04330039024353027\n",
      "Episode: 13 Time: 0.026599931716918945\n",
      "Episode: 14 Time: 0.02160184383392334\n",
      "Episode: 15 Time: 0.029900145530700684\n",
      "Episode: 16 Time: 0.018199753761291505\n",
      "Episode: 17 Time: 0.02319467067718506\n",
      "Episode: 18 Time: 0.04679961204528808\n",
      "Episode: 19 Time: 0.06689972877502441\n",
      "Episode: 20 Time: 0.026499843597412108\n",
      "Episode: 21 Time: 0.06489968299865723\n",
      "Episode: 22 Time: 0.025098133087158202\n",
      "Episode: 23 Time: 0.08169996738433838\n",
      "Episode: 24 Time: 0.05209977626800537\n",
      "Episode: 25 Time: 0.03670015335083008\n",
      "Episode: 26 Time: 0.061499810218811034\n",
      "Episode: 27 Time: 0.01500096321105957\n",
      "Episode: 28 Time: 0.04829971790313721\n",
      "Episode: 29 Time: 0.026596808433532716\n",
      "Episode: 30 Time: 0.06009998321533203\n",
      "Episode: 31 Time: 0.018398809432983398\n",
      "Episode: 32 Time: 0.036602783203125\n",
      "Episode: 33 Time: 0.03830010890960693\n",
      "Episode: 34 Time: 0.07999992370605469\n",
      "Episode: 35 Time: 0.029902553558349608\n",
      "Episode: 36 Time: 0.06330485343933105\n",
      "Episode: 37 Time: 0.05659806728363037\n",
      "Episode: 38 Time: 0.04179978370666504\n",
      "Episode: 39 Time: 0.04330000877380371\n",
      "Episode: 40 Time: 0.021599769592285156\n",
      "Episode: 41 Time: 0.07999980449676514\n",
      "Episode: 42 Time: 0.029965424537658693\n",
      "Episode: 43 Time: 0.028300094604492187\n",
      "Episode: 44 Time: 0.01490170955657959\n",
      "Episode: 45 Time: 0.029897212982177734\n",
      "Episode: 46 Time: 0.029902434349060057\n",
      "Episode: 47 Time: 0.028200411796569826\n",
      "Episode: 48 Time: 0.033297276496887206\n",
      "Episode: 49 Time: 0.026599860191345213\n",
      "Episode: 50 Time: 0.02999744415283203\n",
      "Episode: 51 Time: 0.041699981689453124\n",
      "Episode: 52 Time: 0.06170928478240967\n",
      "Episode: 53 Time: 0.023308372497558592\n",
      "Episode: 54 Time: 0.0266998291015625\n",
      "Episode: 55 Time: 0.02000250816345215\n",
      "Episode: 56 Time: 0.023302626609802247\n",
      "Episode: 57 Time: 0.018296360969543457\n",
      "Episode: 58 Time: 0.019908428192138672\n",
      "Episode: 59 Time: 0.04817914962768555\n",
      "Episode: 60 Time: 0.04489707946777344\n",
      "Episode: 61 Time: 0.021700024604797363\n",
      "Episode: 62 Time: 0.046697449684143064\n",
      "Episode: 63 Time: 0.01990818977355957\n",
      "Episode: 64 Time: 0.026599860191345213\n",
      "Episode: 65 Time: 0.056798577308654785\n",
      "Episode: 66 Time: 0.021701478958129884\n",
      "Episode: 67 Time: 0.028400111198425292\n",
      "Episode: 68 Time: 0.03490417003631592\n",
      "Episode: 69 Time: 0.023299908638000487\n",
      "Episode: 70 Time: 0.03329687118530274\n",
      "Episode: 71 Time: 0.03829920291900635\n",
      "Episode: 72 Time: 0.023397302627563475\n",
      "Episode: 73 Time: 0.02575206756591797\n",
      "Episode: 74 Time: 0.04820005893707276\n",
      "Episode: 75 Time: 0.023401474952697753\n",
      "Episode: 76 Time: 0.0299025297164917\n",
      "Episode: 77 Time: 0.046700024604797365\n",
      "Episode: 78 Time: 0.11500244140625\n",
      "Episode: 79 Time: 0.035100030899047854\n",
      "Episode: 80 Time: 0.026999545097351075\n",
      "Episode: 81 Time: 0.034799814224243164\n",
      "Episode: 82 Time: 0.02669985294342041\n",
      "Episode: 83 Time: 0.046700143814086915\n",
      "Episode: 84 Time: 0.029901981353759766\n",
      "Episode: 85 Time: 0.03000204563140869\n",
      "Episode: 86 Time: 0.02000136375427246\n",
      "Episode: 87 Time: 0.041600275039672854\n",
      "Episode: 88 Time: 0.026700091361999512\n",
      "Episode: 89 Time: 0.050100278854370114\n",
      "Episode: 90 Time: 0.04498422145843506\n",
      "Episode: 91 Time: 0.026700043678283693\n",
      "Episode: 92 Time: 0.023400115966796874\n",
      "Episode: 93 Time: 0.021599817276000976\n",
      "Episode: 94 Time: 0.039902901649475096\n",
      "Episode: 95 Time: 0.02500030994415283\n",
      "Episode: 96 Time: 0.02999866008758545\n",
      "Episode: 97 Time: 0.02170069217681885\n",
      "Episode: 98 Time: 0.028400111198425292\n",
      "Episode: 99 Time: 0.016619133949279784\n",
      "Episode: 100 Time: 0.03579998016357422\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 100\n",
    "n_steps = 10\n",
    "for i in range (n_episodes):\n",
    "    for e in range(n_steps):\n",
    "        # Siscretize state into buckets\n",
    "        current_state, done = discretizer(*env.reset()), False\n",
    "        episode_time_total = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # policy action \n",
    "        while done==False:\n",
    "            action = policy(current_state) # exploit\n",
    "\n",
    "            # insert random action\n",
    "            if np.random.random() < exploration_rate(e) : \n",
    "                action = env.action_space.sample() # explore \n",
    "\n",
    "            # increment enviroment\n",
    "            obs, reward, done, __ = env.step(action)\n",
    "            new_state = discretizer(*obs)\n",
    "\n",
    "            # Update Q-Table\n",
    "            lr = learning_rate(e)\n",
    "            learnt_value = new_Q_value(reward , new_state )\n",
    "            old_value = Q_table[current_state][action]\n",
    "            Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "            # Render the cartpole environment\n",
    "            env.render()\n",
    "\n",
    "        episode_time_total += (time.time()-start_time)\n",
    "    steps_time_average = episode_time_total/n_steps    \n",
    "    print(f\"Episode: {i+1} Time: {steps_time_average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73cdb8068fb29a4d992b50a8130c5699128e49a23c234139f6f5107947219a38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
