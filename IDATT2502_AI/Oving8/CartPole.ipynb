{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import math, random\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation gÃ¥r fra 0-3\n",
    "#0 cart position\n",
    "#1 cart velocity\n",
    "#2 pole angle\n",
    "#3 pole velocity at tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6,12)\n",
    "\n",
    "#upper og lower bounds\n",
    "upper_bounds = [\n",
    "    env.observation_space.high[2], math.radians(50)\n",
    "    ]\n",
    "lower_bounds = [\n",
    "    env.observation_space.low[2], -math.radians(50)\n",
    "    ]\n",
    "Q = np.zeros(n_bins + (n_actions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "n_steps = 10\n",
    "ada_divisor = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy (state : tuple):\n",
    "    return np.argmax(Q[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    future_optimal_value = np.max(Q[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / ada_divisor)))\n",
    "\n",
    "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
    "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / ada_divisor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Time: 0.03837025165557861\n",
      "Episode: 1 Time: 0.11340823173522949\n",
      "Episode: 2 Time: 0.03163573741912842\n",
      "Episode: 3 Time: 0.04171321392059326\n",
      "Episode: 4 Time: 0.06335659027099609\n",
      "Episode: 5 Time: 0.020002198219299317\n",
      "Episode: 6 Time: 0.08842146396636963\n",
      "Episode: 7 Time: 0.07506017684936524\n",
      "Episode: 8 Time: 0.03841292858123779\n",
      "Episode: 9 Time: 0.07151436805725098\n",
      "Episode: 10 Time: 0.021608424186706544\n",
      "Episode: 11 Time: 0.01829988956451416\n",
      "Episode: 12 Time: 0.03159120082855225\n",
      "Episode: 13 Time: 0.03170020580291748\n",
      "Episode: 14 Time: 0.04006791114807129\n",
      "Episode: 15 Time: 0.024875450134277343\n",
      "Episode: 16 Time: 0.03489968776702881\n",
      "Episode: 17 Time: 0.04519529342651367\n",
      "Episode: 18 Time: 0.025100016593933107\n",
      "Episode: 19 Time: 0.02989962100982666\n",
      "Episode: 20 Time: 0.02821199893951416\n",
      "Episode: 21 Time: 0.040077590942382814\n",
      "Episode: 22 Time: 0.01979959011077881\n",
      "Episode: 23 Time: 0.028222250938415527\n",
      "Episode: 24 Time: 0.02167985439300537\n",
      "Episode: 25 Time: 0.020053291320800783\n",
      "Episode: 26 Time: 0.03500843048095703\n",
      "Episode: 27 Time: 0.03835275173187256\n",
      "Episode: 28 Time: 0.0465848445892334\n",
      "Episode: 29 Time: 0.02318694591522217\n",
      "Episode: 30 Time: 0.04993791580200195\n",
      "Episode: 31 Time: 0.015083003044128417\n",
      "Episode: 32 Time: 0.03817441463470459\n",
      "Episode: 33 Time: 0.01827118396759033\n",
      "Episode: 34 Time: 0.04004788398742676\n",
      "Episode: 35 Time: 0.02325406074523926\n",
      "Episode: 36 Time: 0.04003949165344238\n",
      "Episode: 37 Time: 0.028230857849121094\n",
      "Episode: 38 Time: 0.03669288158416748\n",
      "Episode: 39 Time: 0.03151659965515137\n",
      "Episode: 40 Time: 0.06985442638397217\n",
      "Episode: 41 Time: 0.02846236228942871\n",
      "Episode: 42 Time: 0.07082118988037109\n",
      "Episode: 43 Time: 0.04514875411987305\n",
      "Episode: 44 Time: 0.03337278366088867\n",
      "Episode: 45 Time: 0.07005560398101807\n",
      "Episode: 46 Time: 0.03829886913299561\n",
      "Episode: 47 Time: 0.024912476539611816\n",
      "Episode: 48 Time: 0.03829946517944336\n",
      "Episode: 49 Time: 0.023241519927978516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range (n_episodes):\n",
    "    for e in range(n_steps):\n",
    "        # Siscretize state into buckets\n",
    "        current_state, done = discretizer(*env.reset()), False\n",
    "        episode_time_total = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # policy action \n",
    "        while done==False:\n",
    "            action = policy(current_state) # exploit\n",
    "\n",
    "            # insert random action\n",
    "            if np.random.random() < exploration_rate(e) : \n",
    "                action = env.action_space.sample() # explore \n",
    "\n",
    "            # increment enviroment\n",
    "            obs, reward, done, __ = env.step(action)\n",
    "            new_state = discretizer(*obs)\n",
    "\n",
    "            # Update Q-Table\n",
    "            lr = learning_rate(e)\n",
    "            learnt_value = new_Q_value(reward , new_state )\n",
    "            old_value = Q[current_state][action]\n",
    "            Q[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "            # Render the cartpole environment\n",
    "            env.render()\n",
    "\n",
    "        episode_time_total += (time.time()-start_time)\n",
    "    steps_time_average = episode_time_total/n_steps    \n",
    "    print(f\"Episode: {i+1} Time: {steps_time_average}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73cdb8068fb29a4d992b50a8130c5699128e49a23c234139f6f5107947219a38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
