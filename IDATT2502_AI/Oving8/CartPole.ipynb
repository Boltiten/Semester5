{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import math, random\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation gÃ¥r fra 0-3\n",
    "#0 cart position\n",
    "#1 cart velocity\n",
    "#2 pole angle\n",
    "#3 pole velocity at tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6,12)\n",
    "\n",
    "#upper og lower bounds\n",
    "upper_bounds = [\n",
    "    env.observation_space.high[2], math.radians(50)\n",
    "    ]\n",
    "lower_bounds = [\n",
    "    env.observation_space.low[2], -math.radians(50)\n",
    "    ]\n",
    "Q = np.zeros(n_bins + (n_actions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy (state : tuple):\n",
    "    return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    future_optimal_value = np.max(Q[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
    "\n",
    "def exploration_rate(n : int, min_rate=0.15 ) -> float :\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n  + 1) / 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Average time 0.02650015354156494\n",
      "Episode 10 Average time 0.026600122451782227\n",
      "Episode 20 Average time 0.08319942951202393\n",
      "Episode 30 Average time 0.023296928405761717\n",
      "Episode 40 Average time 0.031603527069091794\n",
      "Episode 50 Average time 0.030000519752502442\n",
      "Episode 60 Average time 0.03990323543548584\n",
      "Episode 70 Average time 0.03329977989196777\n",
      "Episode 80 Average time 0.1468001127243042\n",
      "Episode 90 Average time 0.029938125610351564\n",
      "Episode 100 Average time 0.04539999961853027\n",
      "Episode 110 Average time 0.06499791145324707\n",
      "Episode 120 Average time 0.02830023765563965\n",
      "Episode 130 Average time 0.10654630661010742\n",
      "Episode 140 Average time 0.0433002233505249\n",
      "Episode 150 Average time 0.23860445022583007\n",
      "Episode 160 Average time 0.22839994430541993\n",
      "Episode 170 Average time 0.3119048595428467\n",
      "Episode 180 Average time 0.27689993381500244\n",
      "Episode 190 Average time 0.218403959274292\n",
      "Episode 200 Average time 0.21519882678985597\n",
      "Episode 210 Average time 0.05499997138977051\n",
      "Episode 220 Average time 0.3335970401763916\n",
      "Episode 230 Average time 0.3333997011184692\n",
      "Episode 240 Average time 0.24690115451812744\n",
      "Episode 250 Average time 0.3004711627960205\n",
      "Episode 260 Average time 0.3219001054763794\n",
      "Episode 270 Average time 0.1333010673522949\n",
      "Episode 280 Average time 0.32708396911621096\n",
      "Episode 290 Average time 0.11670076847076416\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 300 \n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    # Siscretize state into buckets\n",
    "    current_state, done = discretizer(*env.reset()), False\n",
    "\n",
    "    episode_time = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while done==False:\n",
    "        \n",
    "        # policy action \n",
    "        action = policy(current_state) # exploit\n",
    "        \n",
    "        # insert random action\n",
    "        if np.random.random() < exploration_rate(e) : \n",
    "            action = env.action_space.sample() # explore \n",
    "         \n",
    "        # increment enviroment\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretizer(*obs)\n",
    "        \n",
    "        # Update Q-Table\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward , new_state )\n",
    "        old_value = Q[current_state][action]\n",
    "        Q[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "        \n",
    "        current_state = new_state\n",
    "        \n",
    "        # Render the cartpole environment\n",
    "        env.render()\n",
    "          \n",
    "    \n",
    "    if(done):\n",
    "        episode_time += (time.time()-start_time)  \n",
    "    if((e%10) == 0 & done):\n",
    "        print(f\"Episode {e} Average time {episode_time/10}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73cdb8068fb29a4d992b50a8130c5699128e49a23c234139f6f5107947219a38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
