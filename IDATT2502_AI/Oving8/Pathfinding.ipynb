{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2A. Q-Læring: Gridworld med visualisering\n",
    "Lag et enkelt gridworld-environment. Dette innebærer at environmentet har et\n",
    "diskret rutenett, og at en agent kan bevege seg rundt med fire handlinger (opp,\n",
    "ned, høyre, venstre). Simuleringen terminerer n˚ar agenten har n˚add et plassert\n",
    "m˚al-posisjon som gir reward 1. Om man ønsker, kan det legges inn f.eks. solide\n",
    "vegger eller farlige omr˚ader som gir straff rundt omkring. Environmentet skal\n",
    "ha samme interface som cartpole (.step(a)-funksjon, og .reset())\n",
    "Deretter skal implementasjonen av Q-læring fra forrige oppgave brukes for ˚a\n",
    "trene en agent i environmentet. Til slutt skal Q-verdiene visualiseres inne i selve\n",
    "environmentet, og dette kan gjøres p˚a flere m˚ater. En m˚ate er˚a fargelegge rutene\n",
    "basert p˚a den høyeste Q-verdien fra tilsvarende rad i Q-tabellen. Alternativt s˚a\n",
    "kan man tegne inn piler som peker i samme retning som handlingen med høyest\n",
    "Q-verdi.\n",
    "Tips: Biblioteket pygame er veldig greit for ˚a lage visualisering av environmentet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b7a5e7e088dc87b16d72ecb84aa6dc0c370a308eb78f8481e88a4680c33e6f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
