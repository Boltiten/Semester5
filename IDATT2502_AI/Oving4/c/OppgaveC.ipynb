{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "#Many to many LSTM\r\n",
    "#ta utgangspunkt i rnn/generate-characters og tren modellen p√•\r\n",
    "#bokstavene ‚Äú hello world ‚Äú. Bruk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "class LongShortTermMemoryModel(nn.Module):\r\n",
    "    def __init__(self, encoding_size):\r\n",
    "        super(LongShortTermMemoryModel, self).__init__()\r\n",
    "\r\n",
    "        self.lstm = nn.LSTM(encoding_size, 128)  # 128 is the state size\r\n",
    "        self.dense = nn.Linear(128, encoding_size)  # 128 is the state size\r\n",
    "\r\n",
    "    def reset(self):  # Reset states prior to new input sequence\r\n",
    "        zero_state = torch.zeros(1, 1, 128)  # Shape: (number of layers, batch size, state size)\r\n",
    "        self.hidden_state = zero_state\r\n",
    "        self.cell_state = zero_state\r\n",
    "\r\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\r\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\r\n",
    "        return self.dense(out.reshape(-1, 128))\r\n",
    "\r\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\r\n",
    "        return torch.softmax(self.logits(x), dim=1)\r\n",
    "\r\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\r\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "char_encodings = [\r\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # ' '\r\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'a'\r\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'b'\r\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'c'\r\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'd'\r\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'e'\r\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'f'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'g'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'h'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'i'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'j'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'k'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'l'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'm'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'n'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'o'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'p'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'q'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'r'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 's'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 't'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 'u'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 'v'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 'w'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],  # 'x'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 'y'\r\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]   # 'z'\r\n",
    "    \r\n",
    "    \r\n",
    "]\r\n",
    "encoding_size = len(char_encodings)\r\n",
    "#                 0    1    2    3    4    5    6    7    8    9   10    11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26\r\n",
    "index_to_char = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\r\n",
    "index_to_emojis = ['üé©', 'üêÄ', 'üêà', 'üè¢', 'üöó', 'üë∂', 'üß±']\r\n",
    "emojis = np.eye(len(index_to_emojis))\r\n",
    "\r\n",
    "print(len(char_encodings))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "27\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "x_train = torch.tensor([\r\n",
    "    [[char_encodings[8]],  [char_encodings[1]], [char_encodings[20]], [char_encodings[0]]],     #hat \r\n",
    "    [[char_encodings[18]], [char_encodings[1]], [char_encodings[20]], [char_encodings[0]]],     #rat\r\n",
    "    [[char_encodings[3]],  [char_encodings[1]], [char_encodings[20]], [char_encodings[0]]],     #cat\r\n",
    "    [[char_encodings[6]],  [char_encodings[0]], [char_encodings[1]],  [char_encodings[20]]],    #flat\r\n",
    "    [[char_encodings[3]],  [char_encodings[1]], [char_encodings[18]], [char_encodings[0]]],     #car\r\n",
    "    [[char_encodings[19]], [char_encodings[15]], [char_encodings[14]], [char_encodings[14]]],   #son\r\n",
    "    [[char_encodings[18]], [char_encodings[15]], [char_encodings[3]], [char_encodings[11]]]     #rock\r\n",
    "    ])\r\n",
    "\r\n",
    "y_train = torch.tensor([\r\n",
    "    [emojis[0], emojis[0], emojis[0], emojis[0]], \r\n",
    "    [emojis[1], emojis[1], emojis[1], emojis[1]],\r\n",
    "    [emojis[2], emojis[2], emojis[2], emojis[2]],\r\n",
    "    [emojis[3], emojis[3], emojis[3], emojis[3]],\r\n",
    "    [emojis[4], emojis[4], emojis[4], emojis[4]],\r\n",
    "    [emojis[5], emojis[5], emojis[5], emojis[5]],\r\n",
    "    [emojis[6], emojis[6], emojis[6], emojis[6]]\r\n",
    "    ])\r\n",
    "\r\n",
    "model = LongShortTermMemoryModel(encoding_size)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\r\n",
    "for epoch in range(500):\r\n",
    "    for i in range(x_train.size()[0]):\r\n",
    "        model.reset()\r\n",
    "        model.loss(x_train[i], y_train[i]).backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.zero_grad()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def test_model(input):\r\n",
    "    model.reset()\r\n",
    "    print(\"Input: \" + input)\r\n",
    "    for i in range(len(input)):\r\n",
    "        word = input\r\n",
    "        index = index_to_char.index(word[i])\r\n",
    "        y = model.f(torch.tensor([[char_encodings[index]]]))\r\n",
    "    print(index_to_emojis[y.argmax(1)])\r\n",
    "\r\n",
    "\r\n",
    "test_model(\"rt\")\r\n",
    "test_model(\"ht\")\r\n",
    "test_model(\"sn\")\r\n",
    "test_model(\"ft\")\r\n",
    "test_model(\"rats\")\r\n",
    "test_model(\"rocks\")\r\n",
    "test_model(\"rock n rat\")\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: rt\n",
      "üêÄ\n",
      "Input: ht\n",
      "üé©\n",
      "Input: sn\n",
      "üë∂\n",
      "Input: ft\n",
      "üè¢\n",
      "Input: rats\n",
      "üêÄ\n",
      "Input: rocks\n",
      "üß±\n",
      "Input: rock n rat\n",
      "üë∂\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "73cdb8068fb29a4d992b50a8130c5699128e49a23c234139f6f5107947219a38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}